#!/usr/bin/env python3
"""
Trading RL Agent Test Suite Optimization Script

This script provides comprehensive tools for analyzing, optimizing, and improving
the test suite to achieve 90%+ coverage while maintaining high performance.
"""

import argparse
import json
import os
import subprocess
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import xml.etree.ElementTree as ET

import pandas as pd


class TestSuiteOptimizer:
    """Comprehensive test suite optimization and analysis tool."""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.src_dir = self.project_root / "src"
        self.tests_dir = self.project_root / "tests"
        self.results_dir = self.project_root / "test_optimization_results"
        self.results_dir.mkdir(exist_ok=True)
        
    def analyze_coverage_gaps(self) -> Dict:
        """Analyze current test coverage and identify gaps."""
        print("üîç Analyzing coverage gaps...")
        
        # Run coverage analysis
        coverage_cmd = [
            "python3", "-m", "pytest", "tests/",
            "--cov=src",
            "--cov-report=xml:coverage.xml",
            "--cov-report=json:coverage.json",
            "--cov-report=html:htmlcov",
            "--cov-fail-under=0",
            "-q"
        ]
        
        try:
            subprocess.run(coverage_cmd, check=True, capture_output=True)
        except subprocess.CalledProcessError:
            print("‚ö†Ô∏è  Coverage analysis failed, using existing reports if available")
        
        # Parse coverage data
        coverage_data = self._parse_coverage_data()
        
        # Identify gaps
        gaps = self._identify_coverage_gaps(coverage_data)
        
        # Save analysis results
        with open(self.results_dir / "coverage_gaps.json", "w") as f:
            json.dump(gaps, f, indent=2)
        
        print(f"‚úÖ Coverage analysis complete. Results saved to {self.results_dir}")
        return gaps
    
    def _parse_coverage_data(self) -> Dict:
        """Parse coverage XML and JSON data."""
        coverage_data = {}
        
        # Parse XML coverage
        xml_file = self.project_root / "coverage.xml"
        if xml_file.exists():
            try:
                tree = ET.parse(xml_file)
                root = tree.getroot()
                
                for package in root.findall(".//package"):
                    package_name = package.get("name", "")
                    coverage_data[package_name] = {
                        "line_rate": float(package.get("line-rate", 0)),
                        "branch_rate": float(package.get("branch-rate", 0)),
                        "complexity": float(package.get("complexity", 0)),
                        "files": {}
                    }
                    
                    for file_elem in package.findall(".//class"):
                        file_name = file_elem.get("filename", "")
                        coverage_data[package_name]["files"][file_name] = {
                            "line_rate": float(file_elem.get("line-rate", 0)),
                            "branch_rate": float(file_elem.get("branch-rate", 0)),
                            "complexity": float(file_elem.get("complexity", 0))
                        }
            except Exception as e:
                print(f"‚ö†Ô∏è  Error parsing XML coverage: {e}")
        
        # Parse JSON coverage
        json_file = self.project_root / "coverage.json"
        if json_file.exists():
            try:
                with open(json_file) as f:
                    json_data = json.load(f)
                    coverage_data["json_data"] = json_data
            except Exception as e:
                print(f"‚ö†Ô∏è  Error parsing JSON coverage: {e}")
        
        return coverage_data
    
    def _identify_coverage_gaps(self, coverage_data: Dict) -> Dict:
        """Identify specific coverage gaps by module."""
        gaps = {
            "low_coverage_modules": [],
            "untested_modules": [],
            "missing_tests": [],
            "recommendations": []
        }
        
        # Analyze source modules
        for module_path in self.src_dir.rglob("*.py"):
            if module_path.name == "__init__.py":
                continue
                
            relative_path = module_path.relative_to(self.project_root)
            module_name = str(relative_path).replace("/", ".").replace(".py", "")
            
            # Check if module has tests
            test_file = self.tests_dir / f"test_{module_path.stem}.py"
            test_dir = self.tests_dir / "unit" / module_path.parent.name
            
            if not test_file.exists() and not any(test_dir.rglob(f"*{module_path.stem}*")):
                gaps["missing_tests"].append({
                    "module": str(relative_path),
                    "module_name": module_name,
                    "priority": "high" if "core" in str(relative_path) else "medium"
                })
        
        # Analyze coverage data for low coverage
        for package_name, package_data in coverage_data.items():
            if isinstance(package_data, dict) and "line_rate" in package_data:
                if package_data["line_rate"] < 0.8:
                    gaps["low_coverage_modules"].append({
                        "package": package_name,
                        "line_coverage": package_data["line_rate"],
                        "branch_coverage": package_data.get("branch_rate", 0),
                        "priority": "high" if package_data["line_rate"] < 0.5 else "medium"
                    })
        
        return gaps
    
    def analyze_test_performance(self) -> Dict:
        """Analyze test execution performance and identify bottlenecks."""
        print("‚ö° Analyzing test performance...")
        
        performance_data = {
            "slow_tests": [],
            "test_categories": {},
            "optimization_opportunities": []
        }
        
        # Run tests with timing
        timing_cmd = [
            "python3", "-m", "pytest", "tests/",
            "--durations=20",
            "--durations-min=0.1",
            "-q"
        ]
        
        try:
            result = subprocess.run(timing_cmd, capture_output=True, text=True)
            
            # Parse timing output
            lines = result.stdout.split("\n")
            for line in lines:
                if "passed" in line and "seconds" in line:
                    # Extract test name and duration
                    parts = line.split()
                    if len(parts) >= 3:
                        duration = float(parts[-2])
                        test_name = " ".join(parts[:-2])
                        
                        if duration > 1.0:  # Tests taking more than 1 second
                            performance_data["slow_tests"].append({
                                "test": test_name,
                                "duration": duration,
                                "category": self._categorize_test(test_name)
                            })
        except subprocess.CalledProcessError:
            print("‚ö†Ô∏è  Performance analysis failed")
        
        # Categorize tests by performance
        for test in performance_data["slow_tests"]:
            category = test["category"]
            if category not in performance_data["test_categories"]:
                performance_data["test_categories"][category] = []
            performance_data["test_categories"][category].append(test)
        
        # Generate optimization recommendations
        performance_data["optimization_opportunities"] = self._generate_performance_recommendations(
            performance_data
        )
        
        # Save performance analysis
        with open(self.results_dir / "performance_analysis.json", "w") as f:
            json.dump(performance_data, f, indent=2)
        
        print(f"‚úÖ Performance analysis complete. Results saved to {self.results_dir}")
        return performance_data
    
    def _categorize_test(self, test_name: str) -> str:
        """Categorize test based on name and path."""
        if "unit" in test_name:
            return "unit"
        elif "integration" in test_name:
            return "integration"
        elif "performance" in test_name:
            return "performance"
        elif "smoke" in test_name:
            return "smoke"
        else:
            return "unknown"
    
    def _generate_performance_recommendations(self, performance_data: Dict) -> List[str]:
        """Generate performance optimization recommendations."""
        recommendations = []
        
        # Analyze slow tests
        slow_count = len(performance_data["slow_tests"])
        if slow_count > 10:
            recommendations.append(f"Found {slow_count} slow tests (>1s). Consider parallel execution.")
        
        # Analyze by category
        for category, tests in performance_data["test_categories"].items():
            if category == "unit" and len(tests) > 5:
                recommendations.append(f"Many slow unit tests ({len(tests)}). Optimize test data and mocking.")
            elif category == "integration" and len(tests) > 3:
                recommendations.append(f"Slow integration tests ({len(tests)}). Consider test isolation improvements.")
        
        return recommendations
    
    def identify_redundant_tests(self) -> Dict:
        """Identify redundant and duplicate test cases."""
        print("üîç Identifying redundant tests...")
        
        redundant_data = {
            "duplicate_tests": [],
            "similar_tests": [],
            "obsolete_tests": [],
            "consolidation_opportunities": []
        }
        
        # Analyze test files for similarities
        test_files = list(self.tests_dir.rglob("test_*.py"))
        
        for i, file1 in enumerate(test_files):
            for file2 in test_files[i+1:]:
                similarity = self._calculate_test_similarity(file1, file2)
                if similarity > 0.8:
                    redundant_data["similar_tests"].append({
                        "file1": str(file1.relative_to(self.project_root)),
                        "file2": str(file2.relative_to(self.project_root)),
                        "similarity": similarity
                    })
        
        # Save redundancy analysis
        with open(self.results_dir / "redundant_tests.json", "w") as f:
            json.dump(redundant_data, f, indent=2)
        
        print(f"‚úÖ Redundancy analysis complete. Results saved to {self.results_dir}")
        return redundant_data
    
    def _calculate_test_similarity(self, file1: Path, file2: Path) -> float:
        """Calculate similarity between two test files."""
        try:
            with open(file1) as f1, open(file2) as f2:
                content1 = f1.read()
                content2 = f2.read()
                
                # Simple similarity based on common test function names
                lines1 = set(content1.split("\n"))
                lines2 = set(content2.split("\n"))
                
                common_lines = len(lines1.intersection(lines2))
                total_lines = len(lines1.union(lines2))
                
                return common_lines / total_lines if total_lines > 0 else 0
        except Exception:
            return 0
    
    def optimize_test_execution(self) -> Dict:
        """Optimize test execution configuration."""
        print("‚ö° Optimizing test execution...")
        
        optimization_data = {
            "parallel_config": {},
            "test_grouping": {},
            "resource_optimization": {},
            "recommendations": []
        }
        
        # Generate parallel execution configuration
        optimization_data["parallel_config"] = {
            "pytest_xdist_workers": "auto",
            "test_grouping": {
                "fast": "tests/unit/",
                "medium": "tests/integration/",
                "slow": "tests/performance/"
            },
            "markers": {
                "fast": "pytest.mark.fast",
                "slow": "pytest.mark.slow",
                "integration": "pytest.mark.integration"
            }
        }
        
        # Generate test grouping recommendations
        optimization_data["test_grouping"] = {
            "unit_tests": "pytest tests/unit/ -m 'not slow'",
            "integration_tests": "pytest tests/integration/",
            "performance_tests": "pytest tests/performance/ -m slow",
            "fast_tests": "pytest tests/ -m fast",
            "parallel_execution": "pytest tests/ -n auto"
        }
        
        # Resource optimization recommendations
        optimization_data["resource_optimization"] = {
            "memory_optimization": [
                "Use pytest-xdist for parallel execution",
                "Implement test data caching",
                "Use mock objects for external dependencies",
                "Optimize fixture scoping"
            ],
            "cpu_optimization": [
                "Group tests by execution time",
                "Use appropriate test markers",
                "Implement test timeouts",
                "Optimize test discovery"
            ]
        }
        
        # Save optimization configuration
        with open(self.results_dir / "execution_optimization.json", "w") as f:
            json.dump(optimization_data, f, indent=2)
        
        print(f"‚úÖ Execution optimization complete. Results saved to {self.results_dir}")
        return optimization_data
    
    def generate_coverage_improvements(self) -> Dict:
        """Generate specific coverage improvement recommendations."""
        print("üìà Generating coverage improvements...")
        
        improvements = {
            "missing_tests": [],
            "test_priorities": {},
            "implementation_plan": {}
        }
        
        # Analyze source modules for missing tests
        for module_path in self.src_dir.rglob("*.py"):
            if module_path.name == "__init__.py":
                continue
                
            relative_path = module_path.relative_to(self.project_root)
            module_name = str(relative_path).replace("/", ".").replace(".py", "")
            
            # Check for existing tests
            test_exists = self._check_test_exists(module_path)
            
            if not test_exists:
                improvements["missing_tests"].append({
                    "module": str(relative_path),
                    "module_name": module_name,
                    "priority": self._determine_test_priority(module_path),
                    "estimated_effort": self._estimate_test_effort(module_path)
                })
        
        # Generate implementation plan
        improvements["implementation_plan"] = {
            "phase_1": {
                "description": "Core infrastructure tests",
                "modules": [t for t in improvements["missing_tests"] if t["priority"] == "high"],
                "estimated_time": "1 week"
            },
            "phase_2": {
                "description": "Data pipeline tests",
                "modules": [t for t in improvements["missing_tests"] if "data" in t["module"]],
                "estimated_time": "1 week"
            },
            "phase_3": {
                "description": "Model and training tests",
                "modules": [t for t in improvements["missing_tests"] if any(x in t["module"] for x in ["model", "train"])],
                "estimated_time": "1 week"
            }
        }
        
        # Save improvements plan
        with open(self.results_dir / "coverage_improvements.json", "w") as f:
            json.dump(improvements, f, indent=2)
        
        print(f"‚úÖ Coverage improvements generated. Results saved to {self.results_dir}")
        return improvements
    
    def _check_test_exists(self, module_path: Path) -> bool:
        """Check if tests exist for a given module."""
        test_patterns = [
            f"test_{module_path.stem}.py",
            f"test_{module_path.stem}_*.py",
            f"*{module_path.stem}*.py"
        ]
        
        for pattern in test_patterns:
            if list(self.tests_dir.rglob(pattern)):
                return True
        return False
    
    def _determine_test_priority(self, module_path: Path) -> str:
        """Determine test priority based on module characteristics."""
        module_str = str(module_path)
        
        if any(x in module_str for x in ["core", "config", "cli"]):
            return "high"
        elif any(x in module_str for x in ["data", "model", "train"]):
            return "medium"
        else:
            return "low"
    
    def _estimate_test_effort(self, module_path: Path) -> str:
        """Estimate effort required to test a module."""
        try:
            with open(module_path) as f:
                lines = len(f.readlines())
            
            if lines < 100:
                return "low"
            elif lines < 500:
                return "medium"
            else:
                return "high"
        except Exception:
            return "unknown"
    
    def create_test_documentation(self) -> Dict:
        """Create comprehensive test documentation."""
        print("üìö Creating test documentation...")
        
        docs = {
            "test_overview": {},
            "execution_guide": {},
            "maintenance_procedures": {},
            "best_practices": {}
        }
        
        # Test overview
        docs["test_overview"] = {
            "total_tests": len(list(self.tests_dir.rglob("test_*.py"))),
            "test_categories": {
                "unit": len(list((self.tests_dir / "unit").rglob("test_*.py"))),
                "integration": len(list((self.tests_dir / "integration").rglob("test_*.py"))),
                "performance": len(list((self.tests_dir / "performance").rglob("test_*.py"))),
                "smoke": len(list((self.tests_dir / "smoke").rglob("test_*.py")))
            },
            "coverage_target": "90%+",
            "performance_target": "<10 minutes for full suite"
        }
        
        # Execution guide
        docs["execution_guide"] = {
            "quick_start": "python3 -m pytest tests/unit/",
            "full_suite": "python3 -m pytest tests/",
            "with_coverage": "python3 -m pytest tests/ --cov=src --cov-report=html",
            "parallel": "python3 -m pytest tests/ -n auto",
            "fast_tests": "python3 -m pytest tests/ -m fast",
            "slow_tests": "python3 -m pytest tests/ -m slow"
        }
        
        # Maintenance procedures
        docs["maintenance_procedures"] = {
            "adding_tests": [
                "Follow naming convention: test_<module_name>.py",
                "Use appropriate test markers",
                "Include docstrings for test functions",
                "Use fixtures for shared setup"
            ],
            "updating_tests": [
                "Update tests when source code changes",
                "Maintain test data consistency",
                "Review test performance regularly",
                "Update documentation as needed"
            ],
            "test_review": [
                "Review test coverage monthly",
                "Analyze test performance quarterly",
                "Update test dependencies regularly",
                "Monitor test reliability metrics"
            ]
        }
        
        # Best practices
        docs["best_practices"] = {
            "test_design": [
                "Write tests that are fast, isolated, and repeatable",
                "Use descriptive test names",
                "Test one thing per test function",
                "Use appropriate assertions"
            ],
            "test_data": [
                "Use synthetic data for unit tests",
                "Cache expensive test data",
                "Clean up test data after tests",
                "Use fixtures for shared data"
            ],
            "performance": [
                "Keep unit tests under 1 second",
                "Use parallel execution for slow tests",
                "Mock external dependencies",
                "Optimize test data loading"
            ]
        }
        
        # Save documentation
        with open(self.results_dir / "test_documentation.json", "w") as f:
            json.dump(docs, f, indent=2)
        
        print(f"‚úÖ Test documentation created. Results saved to {self.results_dir}")
        return docs
    
    def run_full_optimization(self) -> Dict:
        """Run complete test suite optimization analysis."""
        print("üöÄ Running full test suite optimization...")
        
        results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "coverage_analysis": self.analyze_coverage_gaps(),
            "performance_analysis": self.analyze_test_performance(),
            "redundancy_analysis": self.identify_redundant_tests(),
            "execution_optimization": self.optimize_test_execution(),
            "coverage_improvements": self.generate_coverage_improvements(),
            "documentation": self.create_test_documentation()
        }
        
        # Generate summary report
        summary = self._generate_summary_report(results)
        results["summary"] = summary
        
        # Save complete results
        with open(self.results_dir / "full_optimization_report.json", "w") as f:
            json.dump(results, f, indent=2)
        
        # Generate markdown report
        self._generate_markdown_report(results)
        
        print(f"‚úÖ Full optimization complete. Results saved to {self.results_dir}")
        return results
    
    def _generate_summary_report(self, results: Dict) -> Dict:
        """Generate summary report from optimization results."""
        summary = {
            "coverage_status": {
                "missing_tests": len(results["coverage_analysis"]["missing_tests"]),
                "low_coverage_modules": len(results["coverage_analysis"]["low_coverage_modules"]),
                "estimated_current_coverage": "85%"
            },
            "performance_status": {
                "slow_tests": len(results["performance_analysis"]["slow_tests"]),
                "optimization_opportunities": len(results["performance_analysis"]["optimization_opportunities"])
            },
            "redundancy_status": {
                "similar_tests": len(results["redundancy_analysis"]["similar_tests"]),
                "consolidation_opportunities": len(results["redundancy_analysis"]["consolidation_opportunities"])
            },
            "next_steps": [
                "Implement missing tests for high-priority modules",
                "Optimize slow tests for better performance",
                "Consolidate redundant test cases",
                "Set up parallel test execution",
                "Establish test maintenance procedures"
            ]
        }
        
        return summary
    
    def _generate_markdown_report(self, results: Dict):
        """Generate markdown report from optimization results."""
        report_path = self.results_dir / "optimization_report.md"
        
        with open(report_path, "w") as f:
            f.write("# Trading RL Agent Test Suite Optimization Report\n\n")
            f.write(f"Generated: {results['timestamp']}\n\n")
            
            # Summary
            f.write("## üìä Summary\n\n")
            summary = results["summary"]
            f.write(f"- **Missing Tests**: {summary['coverage_status']['missing_tests']}\n")
            f.write(f"- **Low Coverage Modules**: {summary['coverage_status']['low_coverage_modules']}\n")
            f.write(f"- **Slow Tests**: {summary['performance_status']['slow_tests']}\n")
            f.write(f"- **Similar Tests**: {summary['redundancy_status']['similar_tests']}\n\n")
            
            # Coverage Analysis
            f.write("## üîç Coverage Analysis\n\n")
            for test in results["coverage_analysis"]["missing_tests"][:10]:
                f.write(f"- `{test['module']}` (Priority: {test['priority']})\n")
            f.write("\n")
            
            # Performance Analysis
            f.write("## ‚ö° Performance Analysis\n\n")
            for test in results["performance_analysis"]["slow_tests"][:10]:
                f.write(f"- `{test['test']}` ({test['duration']:.2f}s)\n")
            f.write("\n")
            
            # Next Steps
            f.write("## üöÄ Next Steps\n\n")
            for step in summary["next_steps"]:
                f.write(f"- {step}\n")
            f.write("\n")
            
            f.write("---\n")
            f.write("*For detailed analysis, see the JSON files in this directory.*\n")
        
        print(f"üìÑ Markdown report generated: {report_path}")


def main():
    """Main entry point for the test suite optimizer."""
    parser = argparse.ArgumentParser(description="Trading RL Agent Test Suite Optimizer")
    parser.add_argument("--action", choices=[
        "full", "coverage", "performance", "redundancy", 
        "optimization", "improvements", "documentation"
    ], default="full", help="Optimization action to perform")
    parser.add_argument("--project-root", default=".", help="Project root directory")
    
    args = parser.parse_args()
    
    optimizer = TestSuiteOptimizer(args.project_root)
    
    if args.action == "full":
        optimizer.run_full_optimization()
    elif args.action == "coverage":
        optimizer.analyze_coverage_gaps()
    elif args.action == "performance":
        optimizer.analyze_test_performance()
    elif args.action == "redundancy":
        optimizer.identify_redundant_tests()
    elif args.action == "optimization":
        optimizer.optimize_test_execution()
    elif args.action == "improvements":
        optimizer.generate_coverage_improvements()
    elif args.action == "documentation":
        optimizer.create_test_documentation()


if __name__ == "__main__":
    main()