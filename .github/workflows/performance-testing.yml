name: Performance Testing & Load Testing

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run performance tests weekly on Sunday at 4 AM UTC
    - cron: '0 4 * * 0'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test to run'
        required: true
        default: 'all'
        type: choice
        options:
          - 'all'
          - 'load'
          - 'stress'
          - 'benchmark'
          - 'memory'
      target_environment:
        description: 'Target environment for testing'
        required: true
        default: 'staging'
        type: choice
        options:
          - 'staging'
          - 'production'

env:
  PYTHON_VERSION: '3.11'
  TEST_DURATION: '10m'
  CONCURRENT_USERS: '100'

jobs:
  # =============================================================================
  # SETUP PERFORMANCE TESTING ENVIRONMENT
  # =============================================================================

  setup-performance-testing:
    name: Setup Performance Testing Environment
    runs-on: ubuntu-latest
    outputs:
      test-url: ${{ steps.setup.outputs.test_url }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install performance testing tools
      run: |
        python -m pip install --upgrade pip
        pip install locust k6-python requests-mock pytest-benchmark memory-profiler psutil

    - name: Setup test environment
      id: setup
      run: |
        if [ "${{ github.event.inputs.target_environment }}" = "production" ]; then
          echo "test_url=https://api.trading-system.com" >> $GITHUB_OUTPUT
        else
          echo "test_url=https://staging-api.trading-system.local" >> $GITHUB_OUTPUT
        fi

  # =============================================================================
  # LOAD TESTING
  # =============================================================================

  load-testing:
    name: Load Testing
    runs-on: ubuntu-latest
    needs: setup-performance-testing
    if: ${{ github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'load' || github.event.inputs.test_type == '' }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Locust
      run: |
        python -m pip install --upgrade pip
        pip install locust

    - name: Create Locust test file
      run: |
        cat > locustfile.py << 'EOF'
        from locust import HttpUser, task, between
        import json
        import random

        class TradingSystemUser(HttpUser):
            wait_time = between(1, 3)
            
            def on_start(self):
                """Initialize user session"""
                self.client.headers.update({
                    'Content-Type': 'application/json',
                    'User-Agent': 'Performance-Test/1.0'
                })
            
            @task(3)
            def health_check(self):
                """Health check endpoint"""
                self.client.get("/health")
            
            @task(2)
            def get_portfolio(self):
                """Get portfolio information"""
                self.client.get("/api/v1/portfolio")
            
            @task(2)
            def get_trading_history(self):
                """Get trading history"""
                self.client.get("/api/v1/trades/history")
            
            @task(1)
            def get_market_data(self):
                """Get market data"""
                symbols = ["AAPL", "GOOGL", "MSFT", "TSLA", "AMZN"]
                symbol = random.choice(symbols)
                self.client.get(f"/api/v1/market/data/{symbol}")
            
            @task(1)
            def submit_order(self):
                """Submit a trading order"""
                order_data = {
                    "symbol": random.choice(["AAPL", "GOOGL", "MSFT"]),
                    "side": random.choice(["buy", "sell"]),
                    "quantity": random.randint(1, 100),
                    "order_type": "market"
                }
                self.client.post("/api/v1/orders", json=order_data)
            
            @task(1)
            def get_performance_metrics(self):
                """Get performance metrics"""
                self.client.get("/api/v1/performance/metrics")
        EOF

    - name: Run load test
      run: |
        locust -f locustfile.py \
          --host=${{ needs.setup-performance-testing.outputs.test_url }} \
          --users=${{ env.CONCURRENT_USERS }} \
          --spawn-rate=10 \
          --run-time=${{ env.TEST_DURATION }} \
          --headless \
          --html=load-test-report.html \
          --csv=load-test-results

    - name: Upload load test results
      uses: actions/upload-artifact@v3
      with:
        name: load-test-results
        path: |
          load-test-report.html
          load-test-results_stats.csv
          load-test-results_stats_history.csv

  # =============================================================================
  # STRESS TESTING
  # =============================================================================

  stress-testing:
    name: Stress Testing
    runs-on: ubuntu-latest
    needs: setup-performance-testing
    if: ${{ github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'stress' || github.event.inputs.test_type == '' }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install stress testing tools
      run: |
        python -m pip install --upgrade pip
        pip install locust requests

    - name: Create stress test file
      run: |
        cat > stress_test.py << 'EOF'
        import requests
        import time
        import threading
        import statistics
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import json

        class StressTest:
            def __init__(self, base_url, max_concurrent=500, duration=300):
                self.base_url = base_url
                self.max_concurrent = max_concurrent
                self.duration = duration
                self.results = []
                self.errors = []
                self.start_time = None
                self.end_time = None
            
            def make_request(self, endpoint, method="GET", data=None):
                """Make a single request and record results"""
                url = f"{self.base_url}{endpoint}"
                start = time.time()
                
                try:
                    if method == "GET":
                        response = requests.get(url, timeout=30)
                    elif method == "POST":
                        response = requests.post(url, json=data, timeout=30)
                    
                    duration = time.time() - start
                    
                    if response.status_code < 500:
                        self.results.append({
                            'endpoint': endpoint,
                            'method': method,
                            'status_code': response.status_code,
                            'duration': duration,
                            'timestamp': start
                        })
                    else:
                        self.errors.append({
                            'endpoint': endpoint,
                            'method': method,
                            'status_code': response.status_code,
                            'duration': duration,
                            'timestamp': start
                        })
                        
                except Exception as e:
                    duration = time.time() - start
                    self.errors.append({
                        'endpoint': endpoint,
                        'method': method,
                        'error': str(e),
                        'duration': duration,
                        'timestamp': start
                    })
            
            def run_stress_test(self):
                """Run the stress test"""
                self.start_time = time.time()
                self.end_time = self.start_time + self.duration
                
                endpoints = [
                    ("/health", "GET"),
                    ("/api/v1/portfolio", "GET"),
                    ("/api/v1/trades/history", "GET"),
                    ("/api/v1/market/data/AAPL", "GET"),
                    ("/api/v1/performance/metrics", "GET")
                ]
                
                print(f"Starting stress test for {self.duration} seconds with {self.max_concurrent} concurrent users")
                
                with ThreadPoolExecutor(max_workers=self.max_concurrent) as executor:
                    while time.time() < self.end_time:
                        futures = []
                        for _ in range(self.max_concurrent):
                            endpoint, method = endpoints[len(futures) % len(endpoints)]
                            if method == "POST":
                                data = {"test": "stress_test"}
                                futures.append(executor.submit(self.make_request, endpoint, method, data))
                            else:
                                futures.append(executor.submit(self.make_request, endpoint, method))
                        
                        # Wait for all requests to complete
                        for future in as_completed(futures):
                            try:
                                future.result()
                            except Exception as e:
                                print(f"Request failed: {e}")
                
                self.generate_report()
            
            def generate_report(self):
                """Generate stress test report"""
                if not self.results:
                    print("No successful requests recorded")
                    return
                
                durations = [r['duration'] for r in self.results]
                status_codes = [r['status_code'] for r in self.results]
                
                report = {
                    'test_duration': self.duration,
                    'max_concurrent_users': self.max_concurrent,
                    'total_requests': len(self.results),
                    'successful_requests': len([r for r in self.results if r['status_code'] < 400]),
                    'failed_requests': len(self.errors),
                    'error_rate': len(self.errors) / (len(self.results) + len(self.errors)) * 100,
                    'response_time_stats': {
                        'mean': statistics.mean(durations),
                        'median': statistics.median(durations),
                        'p95': sorted(durations)[int(len(durations) * 0.95)],
                        'p99': sorted(durations)[int(len(durations) * 0.99)],
                        'min': min(durations),
                        'max': max(durations)
                    },
                    'status_code_distribution': {code: status_codes.count(code) for code in set(status_codes)}
                }
                
                with open('stress-test-report.json', 'w') as f:
                    json.dump(report, f, indent=2)
                
                print("Stress Test Report:")
                print(f"Total Requests: {report['total_requests']}")
                print(f"Successful Requests: {report['successful_requests']}")
                print(f"Failed Requests: {report['failed_requests']}")
                print(f"Error Rate: {report['error_rate']:.2f}%")
                print(f"Mean Response Time: {report['response_time_stats']['mean']:.3f}s")
                print(f"95th Percentile: {report['response_time_stats']['p95']:.3f}s")

        if __name__ == "__main__":
            import sys
            base_url = sys.argv[1] if len(sys.argv) > 1 else "http://localhost:8000"
            test = StressTest(base_url)
            test.run_stress_test()
        EOF

    - name: Run stress test
      run: |
        python stress_test.py ${{ needs.setup-performance-testing.outputs.test_url }}

    - name: Upload stress test results
      uses: actions/upload-artifact@v3
      with:
        name: stress-test-results
        path: stress-test-report.json

  # =============================================================================
  # PERFORMANCE BENCHMARKING
  # =============================================================================

  performance-benchmarking:
    name: Performance Benchmarking
    runs-on: ubuntu-latest
    needs: setup-performance-testing
    if: ${{ github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'benchmark' || github.event.inputs.test_type == '' }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install benchmarking tools
      run: |
        python -m pip install --upgrade pip
        pip install pytest-benchmark requests

    - name: Create benchmark tests
      run: |
        mkdir -p tests/performance
        cat > tests/performance/test_benchmarks.py << 'EOF'
        import pytest
        import requests
        import time
        import statistics
        from concurrent.futures import ThreadPoolExecutor, as_completed

        class TestPerformanceBenchmarks:
            @pytest.fixture(autouse=True)
            def setup(self):
                self.base_url = "${{ needs.setup-performance-testing.outputs.test_url }}"
                self.session = requests.Session()
                self.session.headers.update({
                    'Content-Type': 'application/json',
                    'User-Agent': 'Benchmark-Test/1.0'
                })
            
            def test_health_check_performance(self, benchmark):
                """Benchmark health check endpoint"""
                def health_check():
                    response = self.session.get(f"{self.base_url}/health")
                    return response.status_code == 200
                
                result = benchmark(health_check)
                assert result.status_code == 200
            
            def test_portfolio_api_performance(self, benchmark):
                """Benchmark portfolio API endpoint"""
                def get_portfolio():
                    response = self.session.get(f"{self.base_url}/api/v1/portfolio")
                    return response.status_code == 200
                
                result = benchmark(get_portfolio)
                assert result.status_code == 200
            
            def test_market_data_performance(self, benchmark):
                """Benchmark market data API endpoint"""
                def get_market_data():
                    response = self.session.get(f"{self.base_url}/api/v1/market/data/AAPL")
                    return response.status_code == 200
                
                result = benchmark(get_market_data)
                assert result.status_code == 200
            
            def test_concurrent_requests_performance(self):
                """Test concurrent request performance"""
                endpoints = [
                    "/health",
                    "/api/v1/portfolio",
                    "/api/v1/trades/history",
                    "/api/v1/market/data/AAPL"
                ]
                
                def make_request(endpoint):
                    start = time.time()
                    response = self.session.get(f"{self.base_url}{endpoint}")
                    duration = time.time() - start
                    return {
                        'endpoint': endpoint,
                        'status_code': response.status_code,
                        'duration': duration
                    }
                
                # Test with 50 concurrent requests
                with ThreadPoolExecutor(max_workers=50) as executor:
                    futures = [executor.submit(make_request, endpoint) for endpoint in endpoints * 12]
                    results = [future.result() for future in as_completed(futures)]
                
                durations = [r['duration'] for r in results if r['status_code'] == 200]
                
                # Assert performance requirements
                assert len(durations) > 0, "No successful requests"
                assert statistics.mean(durations) < 1.0, f"Mean response time too high: {statistics.mean(durations):.3f}s"
                assert statistics.median(durations) < 0.5, f"Median response time too high: {statistics.median(durations):.3f}s"
                assert sorted(durations)[int(len(durations) * 0.95)] < 2.0, f"95th percentile too high: {sorted(durations)[int(len(durations) * 0.95)]:.3f}s"
        EOF

    - name: Run performance benchmarks
      run: |
        pytest tests/performance/test_benchmarks.py \
          --benchmark-only \
          --benchmark-json=benchmark-results.json \
          --benchmark-sort=mean \
          -v

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: benchmark-results.json

  # =============================================================================
  # MEMORY AND RESOURCE TESTING
  # =============================================================================

  memory-testing:
    name: Memory & Resource Testing
    runs-on: ubuntu-latest
    needs: setup-performance-testing
    if: ${{ github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'memory' || github.event.inputs.test_type == '' }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install memory profiling tools
      run: |
        python -m pip install --upgrade pip
        pip install memory-profiler psutil requests

    - name: Create memory test script
      run: |
        cat > memory_test.py << 'EOF'
        import requests
        import time
        import psutil
        import json
        from memory_profiler import profile
        import threading
        from concurrent.futures import ThreadPoolExecutor

        class MemoryTest:
            def __init__(self, base_url):
                self.base_url = base_url
                self.session = requests.Session()
                self.session.headers.update({
                    'Content-Type': 'application/json',
                    'User-Agent': 'Memory-Test/1.0'
                })
                self.process = psutil.Process()
            
            def get_memory_usage(self):
                """Get current memory usage"""
                memory_info = self.process.memory_info()
                return {
                    'rss': memory_info.rss / 1024 / 1024,  # MB
                    'vms': memory_info.vms / 1024 / 1024,  # MB
                    'percent': self.process.memory_percent()
                }
            
            @profile
            def test_memory_leak(self):
                """Test for memory leaks with repeated requests"""
                initial_memory = self.get_memory_usage()
                memory_readings = [initial_memory]
                
                print("Testing for memory leaks...")
                
                for i in range(1000):
                    try:
                        response = self.session.get(f"{self.base_url}/health")
                        if i % 100 == 0:
                            memory_readings.append(self.get_memory_usage())
                            print(f"Request {i}: Memory usage: {memory_readings[-1]['rss']:.2f} MB")
                    except Exception as e:
                        print(f"Request {i} failed: {e}")
                
                final_memory = self.get_memory_usage()
                memory_readings.append(final_memory)
                
                # Calculate memory growth
                memory_growth = final_memory['rss'] - initial_memory['rss']
                memory_growth_percent = (memory_growth / initial_memory['rss']) * 100
                
                return {
                    'initial_memory': initial_memory,
                    'final_memory': final_memory,
                    'memory_growth_mb': memory_growth,
                    'memory_growth_percent': memory_growth_percent,
                    'memory_readings': memory_readings
                }
            
            def test_concurrent_memory_usage(self):
                """Test memory usage under concurrent load"""
                initial_memory = self.get_memory_usage()
                
                def make_requests():
                    for _ in range(100):
                        try:
                            self.session.get(f"{self.base_url}/health")
                            self.session.get(f"{self.base_url}/api/v1/portfolio")
                        except Exception as e:
                            print(f"Concurrent request failed: {e}")
                
                # Run concurrent requests
                with ThreadPoolExecutor(max_workers=20) as executor:
                    futures = [executor.submit(make_requests) for _ in range(5)]
                    for future in futures:
                        future.result()
                
                final_memory = self.get_memory_usage()
                
                return {
                    'initial_memory': initial_memory,
                    'final_memory': final_memory,
                    'memory_increase': final_memory['rss'] - initial_memory['rss']
                }
            
            def run_memory_tests(self):
                """Run all memory tests"""
                print("Starting memory tests...")
                
                # Test 1: Memory leak detection
                leak_results = self.test_memory_leak()
                
                # Test 2: Concurrent memory usage
                concurrent_results = self.test_concurrent_memory_usage()
                
                # Generate report
                report = {
                    'timestamp': time.time(),
                    'base_url': self.base_url,
                    'memory_leak_test': leak_results,
                    'concurrent_memory_test': concurrent_results,
                    'system_info': {
                        'cpu_count': psutil.cpu_count(),
                        'memory_total': psutil.virtual_memory().total / 1024 / 1024 / 1024,  # GB
                        'memory_available': psutil.virtual_memory().available / 1024 / 1024 / 1024  # GB
                    }
                }
                
                with open('memory-test-report.json', 'w') as f:
                    json.dump(report, f, indent=2)
                
                print("Memory Test Report:")
                print(f"Memory leak test - Growth: {leak_results['memory_growth_mb']:.2f} MB ({leak_results['memory_growth_percent']:.2f}%)")
                print(f"Concurrent test - Memory increase: {concurrent_results['memory_increase']:.2f} MB")
                
                # Assert memory requirements
                assert leak_results['memory_growth_percent'] < 50, f"Memory growth too high: {leak_results['memory_growth_percent']:.2f}%"
                assert concurrent_results['memory_increase'] < 100, f"Concurrent memory increase too high: {concurrent_results['memory_increase']:.2f} MB"

        if __name__ == "__main__":
            import sys
            base_url = sys.argv[1] if len(sys.argv) > 1 else "http://localhost:8000"
            test = MemoryTest(base_url)
            test.run_memory_tests()
        EOF

    - name: Run memory tests
      run: |
        python memory_test.py ${{ needs.setup-performance-testing.outputs.test_url }}

    - name: Upload memory test results
      uses: actions/upload-artifact@v3
      with:
        name: memory-test-results
        path: memory-test-report.json

  # =============================================================================
  # PERFORMANCE REPORT GENERATION
  # =============================================================================

  generate-performance-report:
    name: Generate Performance Report
    runs-on: ubuntu-latest
    needs: [load-testing, stress-testing, performance-benchmarking, memory-testing]
    if: always()

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download all performance test results
      uses: actions/download-artifact@v3
      with:
        path: performance-results/

    - name: Generate comprehensive performance report
      run: |
        echo "# Performance Test Report" > performance-report.md
        echo "Generated on: $(date)" >> performance-report.md
        echo "Target Environment: ${{ github.event.inputs.target_environment || 'staging' }}" >> performance-report.md
        echo "" >> performance-report.md
        
        echo "## Test Summary" >> performance-report.md
        echo "- Load Testing: ${{ needs.load-testing.result }}" >> performance-report.md
        echo "- Stress Testing: ${{ needs.stress-testing.result }}" >> performance-report.md
        echo "- Performance Benchmarking: ${{ needs.performance-benchmarking.result }}" >> performance-report.md
        echo "- Memory Testing: ${{ needs.memory-testing.result }}" >> performance-report.md
        echo "" >> performance-report.md
        
        # Add detailed results if available
        if [ -f "performance-results/stress-test-results/stress-test-report.json" ]; then
          echo "## Stress Test Results" >> performance-report.md
          cat performance-results/stress-test-results/stress-test-report.json | jq -r '. | "**Total Requests:** \(.total_requests)\n**Error Rate:** \(.error_rate)%\n**Mean Response Time:** \(.response_time_stats.mean)s\n**95th Percentile:** \(.response_time_stats.p95)s"' >> performance-report.md
          echo "" >> performance-report.md
        fi
        
        if [ -f "performance-results/memory-test-results/memory-test-report.json" ]; then
          echo "## Memory Test Results" >> performance-report.md
          cat performance-results/memory-test-results/memory-test-report.json | jq -r '.memory_leak_test | "**Memory Growth:** \(.memory_growth_mb) MB (\(.memory_growth_percent)%)\n**Initial Memory:** \(.initial_memory.rss) MB\n**Final Memory:** \(.final_memory.rss) MB"' >> performance-report.md
          echo "" >> performance-report.md
        fi
        
        echo "## Performance Recommendations" >> performance-report.md
        echo "1. Monitor response times and optimize slow endpoints" >> performance-report.md
        echo "2. Implement caching for frequently accessed data" >> performance-report.md
        echo "3. Consider horizontal scaling for high-traffic periods" >> performance-report.md
        echo "4. Optimize database queries and connection pooling" >> performance-report.md
        echo "5. Implement rate limiting to prevent abuse" >> performance-report.md

    - name: Upload performance report
      uses: actions/upload-artifact@v3
      with:
        name: comprehensive-performance-report
        path: performance-report.md

    - name: Comment performance results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          try {
            const report = fs.readFileSync('performance-report.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
          } catch (error) {
            console.log('Could not read performance report:', error);
          }