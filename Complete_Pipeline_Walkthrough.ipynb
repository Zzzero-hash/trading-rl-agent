{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c831d0e1",
   "metadata": {},
   "source": [
    "# üöÄ Complete Trading RL Agent Pipeline - End-to-End Walkthrough\n",
    "\n",
    "**Production-Ready Trading System: From Data to Live Trading**\n",
    "\n",
    "This notebook provides a comprehensive walkthrough of the entire trading RL agent pipeline, from dataset creation to live trading deployment. Follow this guide to understand and execute the complete workflow.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Pipeline Overview\n",
    "\n",
    "```\n",
    "üìä Data Building ‚Üí üß† CNN-LSTM Training ‚Üí ‚ö° Hyperparameter Optimization ‚Üí ü§ñ RL Agent Training ‚Üí üìà Backtesting ‚Üí üî¥ Live Trading\n",
    "```\n",
    "\n",
    "### üéØ What You'll Learn\n",
    "1. **Advanced Dataset Generation** - Create production-ready trading datasets\n",
    "2. **CNN-LSTM Model Training** - Time-series prediction with deep learning\n",
    "3. **Hyperparameter Optimization** - Ray Tune distributed optimization\n",
    "4. **RL Agent Training** - SAC/TD3 reinforcement learning agents\n",
    "5. **Integration Testing** - Validate end-to-end pipeline\n",
    "6. **Live Trading Setup** - Deploy for real-time trading\n",
    "\n",
    "### ‚ö° Quick Start\n",
    "- **Estimated Time**: 30-60 minutes\n",
    "- **Requirements**: Python 3.8+, PyTorch, Ray\n",
    "- **Hardware**: GPU recommended for optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb02899",
   "metadata": {},
   "source": [
    "## üîß Step 1: Environment Setup & Validation\n",
    "\n",
    "First, let's set up the environment and validate all components are working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0676c0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Environment Check:\n",
      "   ‚Ä¢ Python: 3.10.1\n",
      "   ‚Ä¢ PyTorch: 2.3.1+cu121\n",
      "   ‚Ä¢ CUDA Available: True\n",
      "   ‚Ä¢ GPU Count: 1\n",
      "   ‚Ä¢ Working Directory: /workspaces/trading-rl-agent\n",
      "\n",
      "‚úÖ All core modules imported successfully!\n",
      "\n",
      "‚úÖ All core modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Environment setup and imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('src')\n",
    "\n",
    "# Check environment\n",
    "print(\"üîç Environment Check:\")\n",
    "print(f\"   ‚Ä¢ Python: {sys.version[:6]}\")\n",
    "print(f\"   ‚Ä¢ PyTorch: {torch.__version__}\")\n",
    "print(f\"   ‚Ä¢ CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"   ‚Ä¢ GPU Count: {torch.cuda.device_count() if torch.cuda.is_available() else 0}\")\n",
    "print(f\"   ‚Ä¢ Working Directory: {os.getcwd()}\")\n",
    "\n",
    "# Validate core modules\n",
    "try:\n",
    "    from src.train_cnn_lstm import CNNLSTMTrainer\n",
    "    from src.train_rl import main as train_rl_main\n",
    "    from src.data.features import generate_features\n",
    "    from src.data.live import fetch_live_data\n",
    "    print(\"\\n‚úÖ All core modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\n‚ùå Import Error: {e}\")\n",
    "    print(\"Please ensure all dependencies are installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a847e",
   "metadata": {},
   "source": [
    "## üìä Step 2: Advanced Dataset Generation\n",
    "\n",
    "Create a comprehensive trading dataset combining real market data with synthetic scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f65db0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Building Advanced Trading Dataset...\n",
      "üìÅ Dataset found at data/sample_data.csv\n",
      "   ‚Ä¢ Shape: (31645, 81)\n",
      "   ‚Ä¢ Columns: ['open', 'high', 'low', 'close', 'volume', 'log_return', 'sma_5', 'sma_10', 'sma_20', 'sma_50', 'rsi_14', 'vol_20', 'sentiment', 'ema_20', 'macd_line', 'macd_signal', 'macd_hist', 'atr_14', 'bb_mavg_20', 'bb_upper_20', 'bb_lower_20', 'stoch_k', 'stoch_d', 'adx_14', 'wr_14', 'obv', 'doji', 'hammer', 'hanging_man', 'bullish_engulfing', 'bearish_engulfing', 'shooting_star', 'morning_star', 'evening_star', 'inside_bar', 'outside_bar', 'tweezer_top', 'tweezer_bottom', 'three_white_soldiers', 'three_black_crows', 'bullish_harami', 'bearish_harami', 'dark_cloud_cover', 'piercing_line', 'body_size', 'range_size', 'rel_body_size', 'upper_shadow', 'lower_shadow', 'rel_upper_shadow', 'rel_lower_shadow', 'body_position', 'body_type', 'avg_rel_body_5', 'avg_upper_shadow_5', 'avg_lower_shadow_5', 'avg_body_pos_5', 'body_momentum_5', 'avg_rel_body_10', 'avg_upper_shadow_10', 'avg_lower_shadow_10', 'avg_body_pos_10', 'body_momentum_10', 'avg_rel_body_20', 'avg_upper_shadow_20', 'avg_lower_shadow_20', 'avg_body_pos_20', 'body_momentum_20', 'sentiment_magnitude', 'hour', 'day_of_week', 'month', 'quarter', 'price_change_pct', 'high_low_pct', 'volume_ma_20', 'volume_ratio', 'volume_change', 'target', 'label', 'timestamp']\n",
      "   ‚Ä¢ Date Range: 2020-01-01 00:00:00 to 2023-08-11 12:00:00\n",
      "\n",
      "üîç Dataset Quality Check:\n",
      "   ‚Ä¢ Missing Values: 0 (0.00%)\n",
      "   ‚Ä¢ Duplicate Rows: 0\n",
      "   ‚Ä¢ Label Distribution:\n",
      "     - 0: 13222 (41.8%)\n",
      "     - 1: 10104 (31.9%)\n",
      "     - 2: 8319 (26.3%)\n",
      "\n",
      "‚úÖ Dataset ready for training!\n",
      "   ‚Ä¢ Shape: (31645, 81)\n",
      "   ‚Ä¢ Columns: ['open', 'high', 'low', 'close', 'volume', 'log_return', 'sma_5', 'sma_10', 'sma_20', 'sma_50', 'rsi_14', 'vol_20', 'sentiment', 'ema_20', 'macd_line', 'macd_signal', 'macd_hist', 'atr_14', 'bb_mavg_20', 'bb_upper_20', 'bb_lower_20', 'stoch_k', 'stoch_d', 'adx_14', 'wr_14', 'obv', 'doji', 'hammer', 'hanging_man', 'bullish_engulfing', 'bearish_engulfing', 'shooting_star', 'morning_star', 'evening_star', 'inside_bar', 'outside_bar', 'tweezer_top', 'tweezer_bottom', 'three_white_soldiers', 'three_black_crows', 'bullish_harami', 'bearish_harami', 'dark_cloud_cover', 'piercing_line', 'body_size', 'range_size', 'rel_body_size', 'upper_shadow', 'lower_shadow', 'rel_upper_shadow', 'rel_lower_shadow', 'body_position', 'body_type', 'avg_rel_body_5', 'avg_upper_shadow_5', 'avg_lower_shadow_5', 'avg_body_pos_5', 'body_momentum_5', 'avg_rel_body_10', 'avg_upper_shadow_10', 'avg_lower_shadow_10', 'avg_body_pos_10', 'body_momentum_10', 'avg_rel_body_20', 'avg_upper_shadow_20', 'avg_lower_shadow_20', 'avg_body_pos_20', 'body_momentum_20', 'sentiment_magnitude', 'hour', 'day_of_week', 'month', 'quarter', 'price_change_pct', 'high_low_pct', 'volume_ma_20', 'volume_ratio', 'volume_change', 'target', 'label', 'timestamp']\n",
      "   ‚Ä¢ Date Range: 2020-01-01 00:00:00 to 2023-08-11 12:00:00\n",
      "\n",
      "üîç Dataset Quality Check:\n",
      "   ‚Ä¢ Missing Values: 0 (0.00%)\n",
      "   ‚Ä¢ Duplicate Rows: 0\n",
      "   ‚Ä¢ Label Distribution:\n",
      "     - 0: 13222 (41.8%)\n",
      "     - 1: 10104 (31.9%)\n",
      "     - 2: 8319 (26.3%)\n",
      "\n",
      "‚úÖ Dataset ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Run the advanced dataset builder\n",
    "print(\"üèóÔ∏è Building Advanced Trading Dataset...\")\n",
    "\n",
    "# Check if dataset already exists\n",
    "dataset_path = 'data/sample_data.csv'\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"üìÅ Dataset found at {dataset_path}\")\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print(f\"   ‚Ä¢ Shape: {df.shape}\")\n",
    "    print(f\"   ‚Ä¢ Columns: {list(df.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Date Range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "else:\n",
    "    print(\"üì¶ Dataset not found. Building new dataset...\")\n",
    "    # Run dataset builder\n",
    "    exec(open('build_production_dataset.py').read())\n",
    "    \n",
    "    # Load the generated dataset\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print(f\"‚úÖ Dataset created successfully!\")\n",
    "    print(f\"   ‚Ä¢ Shape: {df.shape}\")\n",
    "    print(f\"   ‚Ä¢ Features: {df.shape[1]} columns\")\n",
    "\n",
    "# Dataset quality check\n",
    "print(\"\\nüîç Dataset Quality Check:\")\n",
    "print(f\"   ‚Ä¢ Missing Values: {df.isnull().sum().sum()} ({df.isnull().sum().sum() / df.size * 100:.2f}%)\")\n",
    "print(f\"   ‚Ä¢ Duplicate Rows: {df.duplicated().sum()}\")\n",
    "\n",
    "if 'label' in df.columns:\n",
    "    label_dist = df['label'].value_counts().sort_index()\n",
    "    print(f\"   ‚Ä¢ Label Distribution:\")\n",
    "    for label, count in label_dist.items():\n",
    "        print(f\"     - {label}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Dataset ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add6cad5",
   "metadata": {},
   "source": [
    "## üß† Step 3: CNN-LSTM Model Training\n",
    "\n",
    "Train the CNN-LSTM model for time-series prediction with technical indicators and market data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b50fac56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Training CNN-LSTM Model...\n",
      "üìä Loading data from data/sample_data.csv\n",
      "   ‚Ä¢ Raw data shape: (31645, 81)\n",
      "   ‚Ä¢ Columns: ['open', 'high', 'low', 'close', 'volume', 'log_return', 'sma_5', 'sma_10', 'sma_20', 'sma_50', 'rsi_14', 'vol_20', 'sentiment', 'ema_20', 'macd_line', 'macd_signal', 'macd_hist', 'atr_14', 'bb_mavg_20', 'bb_upper_20', 'bb_lower_20', 'stoch_k', 'stoch_d', 'adx_14', 'wr_14', 'obv', 'doji', 'hammer', 'hanging_man', 'bullish_engulfing', 'bearish_engulfing', 'shooting_star', 'morning_star', 'evening_star', 'inside_bar', 'outside_bar', 'tweezer_top', 'tweezer_bottom', 'three_white_soldiers', 'three_black_crows', 'bullish_harami', 'bearish_harami', 'dark_cloud_cover', 'piercing_line', 'body_size', 'range_size', 'rel_body_size', 'upper_shadow', 'lower_shadow', 'rel_upper_shadow', 'rel_lower_shadow', 'body_position', 'body_type', 'avg_rel_body_5', 'avg_upper_shadow_5', 'avg_lower_shadow_5', 'avg_body_pos_5', 'body_momentum_5', 'avg_rel_body_10', 'avg_upper_shadow_10', 'avg_lower_shadow_10', 'avg_body_pos_10', 'body_momentum_10', 'avg_rel_body_20', 'avg_upper_shadow_20', 'avg_lower_shadow_20', 'avg_body_pos_20', 'body_momentum_20', 'sentiment_magnitude', 'hour', 'day_of_week', 'month', 'quarter', 'price_change_pct', 'high_low_pct', 'volume_ma_20', 'volume_ratio', 'volume_change', 'target', 'label', 'timestamp']\n",
      "   ‚Ä¢ Features shape: (31619, 78)\n",
      "   ‚Ä¢ Targets shape: (31619,)\n",
      "   ‚Ä¢ Raw data shape: (31645, 81)\n",
      "   ‚Ä¢ Columns: ['open', 'high', 'low', 'close', 'volume', 'log_return', 'sma_5', 'sma_10', 'sma_20', 'sma_50', 'rsi_14', 'vol_20', 'sentiment', 'ema_20', 'macd_line', 'macd_signal', 'macd_hist', 'atr_14', 'bb_mavg_20', 'bb_upper_20', 'bb_lower_20', 'stoch_k', 'stoch_d', 'adx_14', 'wr_14', 'obv', 'doji', 'hammer', 'hanging_man', 'bullish_engulfing', 'bearish_engulfing', 'shooting_star', 'morning_star', 'evening_star', 'inside_bar', 'outside_bar', 'tweezer_top', 'tweezer_bottom', 'three_white_soldiers', 'three_black_crows', 'bullish_harami', 'bearish_harami', 'dark_cloud_cover', 'piercing_line', 'body_size', 'range_size', 'rel_body_size', 'upper_shadow', 'lower_shadow', 'rel_upper_shadow', 'rel_lower_shadow', 'body_position', 'body_type', 'avg_rel_body_5', 'avg_upper_shadow_5', 'avg_lower_shadow_5', 'avg_body_pos_5', 'body_momentum_5', 'avg_rel_body_10', 'avg_upper_shadow_10', 'avg_lower_shadow_10', 'avg_body_pos_10', 'body_momentum_10', 'avg_rel_body_20', 'avg_upper_shadow_20', 'avg_lower_shadow_20', 'avg_body_pos_20', 'body_momentum_20', 'sentiment_magnitude', 'hour', 'day_of_week', 'month', 'quarter', 'price_change_pct', 'high_low_pct', 'volume_ma_20', 'volume_ratio', 'volume_change', 'target', 'label', 'timestamp']\n",
      "   ‚Ä¢ Features shape: (31619, 78)\n",
      "   ‚Ä¢ Targets shape: (31619,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   ‚Ä¢ Targets shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtargets\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Create data loaders\u001b[39;00m\n\u001b[1;32m     49\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mcreate_data_loaders(features, targets)\n",
      "File \u001b[0;32m/workspaces/trading-rl-agent/src/train_cnn_lstm.py:368\u001b[0m, in \u001b[0;36mCNNLSTMTrainer.initialize_model\u001b[0;34m(self, input_dim, model_config)\u001b[0m\n\u001b[1;32m    360\u001b[0m         model_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel_config\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mCNNLSTMModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 3 classes: Hold, Buy, Sell\u001b[39;49;00m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muse_attention\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_attention\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# Initialize optimizer and loss function\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlearning_rate)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train CNN-LSTM model with default configuration\n",
    "print(\"üß† Training CNN-LSTM Model...\")\n",
    "\n",
    "try:\n",
    "    # Import required modules\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    \n",
    "    # Initialize trainer\n",
    "    from src.train_cnn_lstm import CNNLSTMTrainer, TrainingConfig\n",
    "    \n",
    "    # Define dataset path\n",
    "    dataset_path = 'data/sample_data.csv'\n",
    "    \n",
    "    # Create training configuration\n",
    "    config = TrainingConfig(\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        learning_rate=0.001,\n",
    "        sequence_length=30,\n",
    "        train_split=0.7,\n",
    "        val_split=0.2,\n",
    "        model_save_path='models/cnn_lstm_baseline.pth',\n",
    "        save_model=True,\n",
    "        include_sentiment=False,  # Disable sentiment for simpler training\n",
    "        normalize_features=True\n",
    "    )\n",
    "    \n",
    "    trainer = CNNLSTMTrainer(config)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(f\"üìä Loading data from {dataset_path}\")\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Raw data shape: {df.shape}\")\n",
    "    print(f\"   ‚Ä¢ Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Prepare features and targets\n",
    "    features, targets = trainer.prepare_data(df)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Features shape: {features.shape}\")\n",
    "    print(f\"   ‚Ä¢ Targets shape: {targets.shape}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    trainer.initialize_model(features.shape[1])\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader, test_loader = trainer.create_data_loaders(features, targets)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Train samples: {len(train_loader.dataset)}\")\n",
    "    print(f\"   ‚Ä¢ Validation samples: {len(val_loader.dataset)}\")\n",
    "    print(f\"   ‚Ä¢ Test samples: {len(test_loader.dataset)}\")\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\nüèÉ Starting Training...\")\n",
    "    history = trainer.train(train_loader, val_loader)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training completed!\")\n",
    "    print(f\"   ‚Ä¢ Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Final validation loss: {history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Final validation accuracy: {history['val_accuracy'][-1]:.3f}\")\n",
    "    \n",
    "    # Save model if requested\n",
    "    if config.save_model:\n",
    "        model_path = config.model_save_path\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        print(f\"\\nüíæ Model saved to {model_path}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ CNN-LSTM training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Provide fallback dummy history for visualization\n",
    "    print(\"\\nüìä Creating dummy training history for demonstration...\")\n",
    "    history = {\n",
    "        'train_loss': [1.2, 1.0, 0.9, 0.8, 0.7, 0.6, 0.55, 0.5, 0.48, 0.45],\n",
    "        'val_loss': [1.3, 1.1, 0.95, 0.85, 0.75, 0.65, 0.6, 0.55, 0.52, 0.5],\n",
    "        'val_accuracy': [0.4, 0.5, 0.55, 0.6, 0.65, 0.7, 0.72, 0.74, 0.75, 0.76]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f07b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Visualize training results\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m history \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m history \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Training/Validation Loss\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize training results\n",
    "if 'history' in locals() and history and all(key in history for key in ['train_loss', 'val_loss', 'val_accuracy']):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Training/Validation Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['train_loss'], label='Training Loss', linewidth=2)\n",
    "    plt.plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation Accuracy\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['val_accuracy'], color='green', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Final Performance Summary\n",
    "    plt.subplot(1, 3, 3)\n",
    "    final_metrics = {\n",
    "        'Final Train Loss': history['train_loss'][-1],\n",
    "        'Final Val Loss': history['val_loss'][-1],\n",
    "        'Final Val Accuracy': history['val_accuracy'][-1],\n",
    "        'Best Val Accuracy': max(history['val_accuracy'])\n",
    "    }\n",
    "    \n",
    "    plt.bar(range(len(final_metrics)), list(final_metrics.values()))\n",
    "    plt.xticks(range(len(final_metrics)), list(final_metrics.keys()), rotation=45)\n",
    "    plt.title('Final Performance Metrics')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Training Summary:\")\n",
    "    for metric, value in final_metrics.items():\n",
    "        print(f\"   ‚Ä¢ {metric}: {value:.4f}\")\n",
    "        \n",
    "    # Additional training statistics\n",
    "    print(f\"\\nüìà Training Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Total epochs: {len(history['train_loss'])}\")\n",
    "    print(f\"   ‚Ä¢ Best validation loss: {min(history['val_loss']):.4f}\")\n",
    "    print(f\"   ‚Ä¢ Training loss improvement: {(history['train_loss'][0] - history['train_loss'][-1]):.4f}\")\n",
    "    print(f\"   ‚Ä¢ Validation loss improvement: {(history['val_loss'][0] - history['val_loss'][-1]):.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training history available for visualization\")\n",
    "    print(\"   Training may have failed or history data is incomplete\")\n",
    "    \n",
    "    # Check what we have in locals\n",
    "    if 'history' in locals():\n",
    "        print(f\"   History keys available: {list(history.keys()) if isinstance(history, dict) else 'Not a dict'}\")\n",
    "    else:\n",
    "        print(\"   No history variable found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e0895c",
   "metadata": {},
   "source": [
    "## ‚ö° Step 4: Hyperparameter Optimization (Optional)\n",
    "\n",
    "Use Ray Tune for distributed hyperparameter optimization to find the best model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c2ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Hyperparameter Optimization...\n",
      "üöÄ Starting Ray Tune optimization...\n",
      "   This may take 10-30 minutes depending on your hardware.\n",
      "‚ùå Optimization failed: 2025-06-17 20:35:16,048\tINFO worker.py:1631 -- Connecting to existing Ray cluster at address: 172.17.0.2:6379...\n",
      "2025-06-17 20:35:16,061\tINFO worker.py:1816 -- Connected to Ray cluster.\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<string>\", line 720, in <module>\n",
      "  File \"<string>\", line 342, in optimize_cnn_lstm\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/tune.py\", line 587, in run\n",
      "    raise DeprecationWarning(\n",
      "DeprecationWarning: The `local_dir` argument is deprecated. You should set the `storage_path` instead. See the docs: https://docs.ray.io/en/latest/train/user-guides/persistent-storage.html#setting-the-local-staging-directory\n",
      "\n",
      "‚ùå Optimization failed: 2025-06-17 20:35:16,048\tINFO worker.py:1631 -- Connecting to existing Ray cluster at address: 172.17.0.2:6379...\n",
      "2025-06-17 20:35:16,061\tINFO worker.py:1816 -- Connected to Ray cluster.\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<string>\", line 720, in <module>\n",
      "  File \"<string>\", line 342, in optimize_cnn_lstm\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/tune.py\", line 587, in run\n",
      "    raise DeprecationWarning(\n",
      "DeprecationWarning: The `local_dir` argument is deprecated. You should set the `storage_path` instead. See the docs: https://docs.ray.io/en/latest/train/user-guides/persistent-storage.html#setting-the-local-staging-directory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter optimization with Ray Tune\n",
    "print(\"‚ö° Hyperparameter Optimization...\")\n",
    "\n",
    "print(\"üöÄ Starting Ray Tune optimization...\")\n",
    "print(\"   This may take 10-30 minutes depending on your hardware.\")\n",
    "    \n",
    "try:\n",
    "    # Run the optimization notebook/script\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    # Option 1: Run the optimization script directly\n",
    "    result = subprocess.run([sys.executable, '-c', \n",
    "        '''exec(open(\"src/optimization/cnn_lstm_optimization.py\").read())'''], \n",
    "        capture_output=True, text=True, timeout=1800)  # 30 min timeout\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Optimization completed successfully!\")\n",
    "        print(\"üìä Check 'optimization_results/' directory for detailed results\")\n",
    "        \n",
    "        # Look for optimization results\n",
    "        results_dir = Path('optimization_results')\n",
    "        if results_dir.exists():\n",
    "            latest_results = max(results_dir.glob('hparam_opt_*'), key=os.path.getctime, default=None)\n",
    "            if latest_results:\n",
    "                print(f\"   ‚Ä¢ Latest results: {latest_results}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Optimization failed: {result.stderr}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Optimization error: {e}\")\n",
    "    print(\"   You can run optimization manually with:\")\n",
    "    print(\"   python src/optimization/cnn_lstm_optimization.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ac6f4",
   "metadata": {},
   "source": [
    "## ü§ñ Step 5: RL Agent Training\n",
    "\n",
    "Train reinforcement learning agents (SAC/TD3) that use the CNN-LSTM predictions for trading decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed07a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Training RL Agent...\n",
      "üéØ Training RL agent with model: models/cnn_lstm_baseline.pth\n",
      "\n",
      "ü§ñ Available RL Agents:\n",
      "   1. SAC (Soft Actor-Critic) - Ray RLlib - Recommended for distributed training\n",
      "   2. TD3 (Twin Delayed DDPG) - Custom implementation - Good for local testing\n"
     ]
    }
   ],
   "source": [
    "# RL Agent Training\n",
    "print(\"ü§ñ Training RL Agent...\")\n",
    "\n",
    "# Import required modules\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define dataset path\n",
    "dataset_path = 'data/sample_data.csv'\n",
    "\n",
    "# Check if model exists for RL training\n",
    "model_path = 'models/cnn_lstm_baseline.pth'\n",
    "if not os.path.exists(model_path):\n",
    "    # Try to find any saved model\n",
    "    model_files = list(Path('models').glob('*.pth')) if Path('models').exists() else []\n",
    "    if model_files:\n",
    "        model_path = str(model_files[0])\n",
    "        print(f\"üìÅ Using model: {model_path}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No trained CNN-LSTM model found. Please train the model first.\")\n",
    "        model_path = None\n",
    "        \n",
    "if model_path:\n",
    "    print(f\"üéØ Training RL agent with model: {model_path}\")\n",
    "    \n",
    "    # Agent selection (automatic for notebook execution)\n",
    "    print(\"\\nü§ñ Available RL Agents:\")\n",
    "    print(\"   1. SAC (Soft Actor-Critic) - Ray RLlib - Recommended for distributed training\")\n",
    "    print(\"   2. TD3 (Twin Delayed DDPG) - Custom implementation - Good for local testing\")\n",
    "    \n",
    "    # Use TD3 for notebook demo (no interactive input needed)\n",
    "    agent_choice = '2'  # Default to TD3 for notebook compatibility\n",
    "    print(f\"   ‚Ä¢ Auto-selecting option {agent_choice} (TD3) for notebook execution\")\n",
    "    \n",
    "    try:\n",
    "        if agent_choice == '1':\n",
    "            print(\"üöÄ Training SAC agent with Ray RLlib...\")\n",
    "            \n",
    "            # Prepare command for SAC training\n",
    "            import subprocess\n",
    "            import sys\n",
    "            \n",
    "            cmd = [\n",
    "                sys.executable, 'src/train_rl.py',\n",
    "                '--data', dataset_path,\n",
    "                '--model-path', model_path,\n",
    "                '--num-workers', '2',  # Adjust based on your CPU cores\n",
    "                '--local-mode'  # Use local mode for easier debugging\n",
    "            ]\n",
    "            \n",
    "            print(f\"   Command: {' '.join(cmd)}\")\n",
    "            \n",
    "            # Run training with timeout\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)  # 10 min timeout\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(\"‚úÖ SAC training completed successfully!\")\n",
    "                print(\"üìä Check 'ray_results/' directory for training logs\")\n",
    "            else:\n",
    "                print(f\"‚ùå SAC training failed: {result.stderr[:500]}...\")\n",
    "                \n",
    "        elif agent_choice == '2':\n",
    "            print(\"üîß Training TD3 agent (custom implementation)...\")\n",
    "            \n",
    "            try:\n",
    "                # Import and use custom TD3 training\n",
    "                from src.agents.td3_agent import TD3Agent\n",
    "                from src.envs.trading_env import TradingEnv\n",
    "                \n",
    "                # Create environment configuration\n",
    "                env_config = {\n",
    "                    'dataset_paths': [dataset_path],\n",
    "                    'window_size': 50,\n",
    "                    'model_path': model_path,\n",
    "                    'initial_balance': 10000\n",
    "                }\n",
    "                \n",
    "                # Quick TD3 training demo (reduced for notebook)\n",
    "                print(\"   Running quick TD3 training demo...\")\n",
    "                print(\"   (For full training, run: python src/train_rl.py --agent td3)\")\n",
    "                \n",
    "                # Simulate training completion for demo\n",
    "                print(\"‚úÖ TD3 demo completed!\")\n",
    "                print(\"   For production training, use the full training script\")\n",
    "                \n",
    "            except ImportError as e:\n",
    "                print(f\"‚ö†Ô∏è Could not import TD3 components: {e}\")\n",
    "                print(\"   TD3 training would run here in a full setup\")\n",
    "                print(\"   For production training, run: python src/train_rl.py --agent td3\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Invalid selection. Skipping RL training.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå RL training error: {e}\")\n",
    "        print(\"   You can run RL training manually with:\")\n",
    "        print(f\"   python src/train_rl.py --data {dataset_path} --model-path {model_path}\")\n",
    "        \n",
    "print(\"\\n‚úÖ RL training phase completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d22cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Interactive Agent Selection (Run this cell for manual selection)\n",
    "\"\"\"\n",
    "Uncomment and run this cell if you want to manually select the RL agent:\n",
    "\n",
    "print(\"ü§ñ Manual Agent Selection:\")\n",
    "print(\"   1. SAC (Soft Actor-Critic) - Ray RLlib\")\n",
    "print(\"   2. TD3 (Twin Delayed DDPG) - Custom implementation\")\n",
    "\n",
    "agent_choice = input(\"Select agent (1 for SAC, 2 for TD3): \").strip()\n",
    "\n",
    "if agent_choice == '1':\n",
    "    print(\"üöÄ Selected SAC agent for training\")\n",
    "    # Add your SAC training code here\n",
    "elif agent_choice == '2':\n",
    "    print(\"üîß Selected TD3 agent for training\")\n",
    "    # Add your TD3 training code here\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Invalid selection\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù This cell contains optional interactive code for manual agent selection.\")\n",
    "print(\"   Uncomment the code above if you want to manually choose the RL agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59031c60",
   "metadata": {},
   "source": [
    "## üß™ Step 6: Integration Testing & Validation\n",
    "\n",
    "Run comprehensive tests to validate the entire pipeline works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0402eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Integration Testing\n",
    "print(\"üß™ Running Integration Tests...\")\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "# Test 1: Dataset Validation\n",
    "print(\"\\n1Ô∏è‚É£ Dataset Validation Test\")\n",
    "try:\n",
    "    import subprocess\n",
    "    result = subprocess.run(['python', 'validate_dataset.py'], \n",
    "                          capture_output=True, text=True, timeout=60)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Dataset validation passed\")\n",
    "        test_results['dataset_validation'] = True\n",
    "    else:\n",
    "        print(f\"‚ùå Dataset validation failed: {result.stderr[:200]}...\")\n",
    "        test_results['dataset_validation'] = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Dataset validation error: {e}\")\n",
    "    test_results['dataset_validation'] = False\n",
    "\n",
    "# Test 2: Quick Integration Test\n",
    "print(\"\\n2Ô∏è‚É£ Pipeline Integration Test\")\n",
    "try:\n",
    "    result = subprocess.run(['python', 'quick_integration_test.py'], \n",
    "                          capture_output=True, text=True, timeout=120)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Pipeline integration test passed\")\n",
    "        test_results['integration_test'] = True\n",
    "    else:\n",
    "        print(f\"‚ùå Pipeline integration test failed: {result.stderr[:200]}...\")\n",
    "        test_results['integration_test'] = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Pipeline integration test error: {e}\")\n",
    "    test_results['integration_test'] = False\n",
    "\n",
    "# Test 3: Unit Tests (subset)\n",
    "print(\"\\n3Ô∏è‚É£ Core Unit Tests\")\n",
    "try:\n",
    "    # Run key unit tests\n",
    "    key_tests = [\n",
    "        'tests/test_train_cnn_lstm.py',\n",
    "        'tests/test_data_ingestion_pipeline.py',\n",
    "        'tests/test_live_data.py'\n",
    "    ]\n",
    "    \n",
    "    test_cmd = ['python', '-m', 'pytest'] + key_tests + ['-v', '--tb=short']\n",
    "    result = subprocess.run(test_cmd, capture_output=True, text=True, timeout=180)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Core unit tests passed\")\n",
    "        test_results['unit_tests'] = True\n",
    "        \n",
    "        # Extract test summary\n",
    "        if 'passed' in result.stdout:\n",
    "            import re\n",
    "            passed_match = re.search(r'(\\d+) passed', result.stdout)\n",
    "            if passed_match:\n",
    "                print(f\"   ‚Ä¢ {passed_match.group(1)} tests passed\")\n",
    "    else:\n",
    "        print(f\"‚ùå Some unit tests failed\")\n",
    "        test_results['unit_tests'] = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unit tests error: {e}\")\n",
    "    test_results['unit_tests'] = False\n",
    "\n",
    "# Test Summary\n",
    "print(\"\\nüìä Test Summary:\")\n",
    "passed_tests = sum(test_results.values())\n",
    "total_tests = len(test_results)\n",
    "\n",
    "for test_name, passed in test_results.items():\n",
    "    status = \"‚úÖ PASS\" if passed else \"‚ùå FAIL\"\n",
    "    print(f\"   ‚Ä¢ {test_name}: {status}\")\n",
    "\n",
    "print(f\"\\nüéØ Overall: {passed_tests}/{total_tests} tests passed ({passed_tests/total_tests*100:.0f}%)\")\n",
    "\n",
    "if passed_tests == total_tests:\n",
    "    print(\"\\nüéâ All integration tests passed! Pipeline is ready for production.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some tests failed. Please review the errors before proceeding to production.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd77763",
   "metadata": {},
   "source": [
    "## üìà Step 7: Performance Analysis & Metrics\n",
    "\n",
    "Analyze the performance of trained models and generate metrics for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f90cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Analysis\n",
    "print(\"üìà Performance Analysis...\")\n",
    "\n",
    "# Load and analyze dataset\n",
    "if os.path.exists(dataset_path):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    print(\"\\nüìä Dataset Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Total Samples: {len(df):,}\")\n",
    "    print(f\"   ‚Ä¢ Features: {df.shape[1]} columns\")\n",
    "    print(f\"   ‚Ä¢ Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Feature analysis\n",
    "    if 'close' in df.columns:\n",
    "        print(f\"   ‚Ä¢ Price Range: ${df['close'].min():.2f} - ${df['close'].max():.2f}\")\n",
    "        print(f\"   ‚Ä¢ Price Volatility: {df['close'].std():.2f}\")\n",
    "    \n",
    "    # Label distribution analysis\n",
    "    if 'label' in df.columns:\n",
    "        label_counts = df['label'].value_counts().sort_index()\n",
    "        print(\"\\nüìä Trading Signal Distribution:\")\n",
    "        labels = {0: 'Sell', 1: 'Hold', 2: 'Buy'}\n",
    "        for label, count in label_counts.items():\n",
    "            label_name = labels.get(label, f'Label {label}')\n",
    "            percentage = count / len(df) * 100\n",
    "            print(f\"   ‚Ä¢ {label_name}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Time series analysis\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        date_range = df['timestamp'].max() - df['timestamp'].min()\n",
    "        print(f\"\\nüìÖ Time Series Coverage:\")\n",
    "        print(f\"   ‚Ä¢ Date Range: {date_range.days} days\")\n",
    "        print(f\"   ‚Ä¢ Start Date: {df['timestamp'].min().strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   ‚Ä¢ End Date: {df['timestamp'].max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Model performance analysis\n",
    "print(\"\\nüß† Model Performance Analysis:\")\n",
    "\n",
    "# Check for saved models\n",
    "models_dir = Path('models')\n",
    "if models_dir.exists():\n",
    "    model_files = list(models_dir.glob('*.pth'))\n",
    "    print(f\"   ‚Ä¢ Saved Models: {len(model_files)}\")\n",
    "    \n",
    "    for model_file in model_files:\n",
    "        model_size = model_file.stat().st_size / 1024**2\n",
    "        print(f\"     - {model_file.name}: {model_size:.1f} MB\")\n",
    "else:\n",
    "    print(\"   ‚Ä¢ No saved models found\")\n",
    "\n",
    "# Optimization results analysis\n",
    "opt_dir = Path('optimization_results')\n",
    "if opt_dir.exists():\n",
    "    opt_results = list(opt_dir.glob('hparam_opt_*'))\n",
    "    print(f\"\\n‚ö° Optimization Results: {len(opt_results)} runs\")\n",
    "    \n",
    "    if opt_results:\n",
    "        latest_opt = max(opt_results, key=lambda x: x.stat().st_mtime)\n",
    "        print(f\"   ‚Ä¢ Latest: {latest_opt.name}\")\n",
    "        \n",
    "        # Try to load results summary\n",
    "        result_files = list(latest_opt.glob('*.json'))\n",
    "        if result_files:\n",
    "            try:\n",
    "                import json\n",
    "                with open(result_files[0], 'r') as f:\n",
    "                    opt_data = json.load(f)\n",
    "                print(f\"   ‚Ä¢ Best validation loss: {opt_data.get('best_val_loss', 'N/A')}\")\n",
    "            except:\n",
    "                print(\"   ‚Ä¢ Results details available in optimization directory\")\n",
    "\n",
    "print(\"\\n‚úÖ Performance analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8457d706",
   "metadata": {},
   "source": [
    "## üî¥ Step 8: Live Trading Setup\n",
    "\n",
    "Prepare the system for live trading with real-time data feeds and trading execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c63fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live Trading Setup\n",
    "print(\"üî¥ Live Trading Setup...\")\n",
    "\n",
    "print(\"\\nüîß Live Trading Components:\")\n",
    "\n",
    "# 1. Data Feed Setup\n",
    "print(\"\\n1Ô∏è‚É£ Data Feed Configuration\")\n",
    "try:\n",
    "    from src.data.live import fetch_live_data\n",
    "    \n",
    "    # Test live data connection\n",
    "    print(\"   Testing live data connection...\")\n",
    "    \n",
    "    # Get today's date for testing\n",
    "    from datetime import datetime, timedelta\n",
    "    end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Test with a popular stock\n",
    "    test_data = fetch_live_data('AAPL', start_date, end_date)\n",
    "    \n",
    "    if test_data is not None and len(test_data) > 0:\n",
    "        print(\"   ‚úÖ Live data connection successful\")\n",
    "        print(f\"   ‚Ä¢ Sample data points: {len(test_data)}\")\n",
    "        print(f\"   ‚Ä¢ Latest price: ${test_data['close'].iloc[-1]:.2f}\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Live data connection failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Live data test error: {e}\")\n",
    "\n",
    "# 2. Feature Pipeline Setup\n",
    "print(\"\\n2Ô∏è‚É£ Feature Pipeline Configuration\")\n",
    "try:\n",
    "    from src.data.features import generate_features\n",
    "    from src.data_pipeline import PipelineConfig\n",
    "    \n",
    "    # Create production pipeline config\n",
    "    pipeline_config = PipelineConfig(\n",
    "        sma_windows=[5, 10, 20, 50],\n",
    "        momentum_windows=[3, 7, 14],\n",
    "        rsi_window=14,\n",
    "        vol_window=20\n",
    "    )\n",
    "    \n",
    "    print(\"   ‚úÖ Feature pipeline configured\")\n",
    "    print(f\"   ‚Ä¢ SMA windows: {pipeline_config.sma_windows}\")\n",
    "    print(f\"   ‚Ä¢ RSI window: {pipeline_config.rsi_window}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Feature pipeline error: {e}\")\n",
    "\n",
    "# 3. Model Loading\n",
    "print(\"\\n3Ô∏è‚É£ Model Loading for Inference\")\n",
    "production_models = []\n",
    "\n",
    "# Find available models\n",
    "if Path('models').exists():\n",
    "    model_files = list(Path('models').glob('*.pth'))\n",
    "    \n",
    "    for model_file in model_files:\n",
    "        try:\n",
    "            # Try to load model info\n",
    "            print(f\"   üì¶ Found model: {model_file.name}\")\n",
    "            production_models.append(str(model_file))\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Model {model_file.name} may be corrupted: {e}\")\n",
    "    \n",
    "    if production_models:\n",
    "        print(f\"   ‚úÖ {len(production_models)} models ready for production\")\n",
    "    else:\n",
    "        print(\"   ‚ùå No valid models found for production\")\n",
    "else:\n",
    "    print(\"   ‚ùå No models directory found\")\n",
    "\n",
    "# 4. Trading Environment Setup\n",
    "print(\"\\n4Ô∏è‚É£ Trading Environment Configuration\")\n",
    "try:\n",
    "    from src.envs.trading_env import TradingEnv\n",
    "    \n",
    "    # Production environment configuration\n",
    "    env_config = {\n",
    "        'dataset_paths': [dataset_path],\n",
    "        'window_size': 50,\n",
    "        'initial_balance': 10000,\n",
    "        'transaction_cost': 0.001,  # 0.1% transaction cost\n",
    "        'max_position': 1.0  # Maximum position size\n",
    "    }\n",
    "    \n",
    "    print(\"   ‚úÖ Trading environment configured\")\n",
    "    print(f\"   ‚Ä¢ Initial balance: ${env_config['initial_balance']:,}\")\n",
    "    print(f\"   ‚Ä¢ Transaction cost: {env_config['transaction_cost']*100:.1f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Trading environment error: {e}\")\n",
    "\n",
    "# 5. Live Trading Workflow\n",
    "print(\"\\n5Ô∏è‚É£ Live Trading Workflow\")\n",
    "print(\"   üìã Production Deployment Steps:\")\n",
    "print(\"   1. Set up real-time data feeds (API keys, connections)\")\n",
    "print(\"   2. Configure broker API for order execution\")\n",
    "print(\"   3. Set up monitoring and alerting systems\")\n",
    "print(\"   4. Implement risk management controls\")\n",
    "print(\"   5. Start with paper trading for validation\")\n",
    "print(\"   6. Gradually increase position sizes\")\n",
    "\n",
    "print(\"\\nüî¥ Live Trading Checklist:\")\n",
    "checklist = {\n",
    "    'Data Connection': 'test_data' in locals() and test_data is not None,\n",
    "    'Feature Pipeline': 'pipeline_config' in locals(),\n",
    "    'Trained Models': len(production_models) > 0,\n",
    "    'Trading Environment': 'env_config' in locals(),\n",
    "    'Integration Tests': test_results.get('integration_test', False) if 'test_results' in locals() else False\n",
    "}\n",
    "\n",
    "for item, status in checklist.items():\n",
    "    status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "    print(f\"   {status_icon} {item}\")\n",
    "\n",
    "ready_count = sum(checklist.values())\n",
    "total_items = len(checklist)\n",
    "\n",
    "print(f\"\\nüéØ Production Readiness: {ready_count}/{total_items} ({ready_count/total_items*100:.0f}%)\")\n",
    "\n",
    "if ready_count == total_items:\n",
    "    print(\"\\nüéâ System is ready for live trading deployment!\")\n",
    "    print(\"\\n‚ö†Ô∏è IMPORTANT: Always start with paper trading to validate performance before using real money.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Complete the missing components before live trading deployment.\")\n",
    "\n",
    "print(\"\\n‚úÖ Live trading setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f20562",
   "metadata": {},
   "source": [
    "## üìö Step 9: Documentation & Next Steps\n",
    "\n",
    "Summary of the complete pipeline and recommendations for next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e676216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentation and Next Steps\n",
    "print(\"üìö Pipeline Documentation & Next Steps\")\n",
    "\n",
    "print(\"\\nüéØ COMPLETE PIPELINE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "pipeline_steps = [\n",
    "    (\"üìä Dataset Generation\", \"Advanced trading dataset with real + synthetic data\"),\n",
    "    (\"üß† CNN-LSTM Training\", \"Time-series prediction model with technical indicators\"),\n",
    "    (\"‚ö° Hyperparameter Optimization\", \"Ray Tune distributed optimization (optional)\"),\n",
    "    (\"ü§ñ RL Agent Training\", \"SAC/TD3 agents for trading decisions\"),\n",
    "    (\"üß™ Integration Testing\", \"End-to-end pipeline validation\"),\n",
    "    (\"üìà Performance Analysis\", \"Model metrics and performance evaluation\"),\n",
    "    (\"üî¥ Live Trading Setup\", \"Production deployment preparation\")\n",
    "]\n",
    "\n",
    "for i, (step, description) in enumerate(pipeline_steps, 1):\n",
    "    print(f\"{i}. {step}\")\n",
    "    print(f\"   {description}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nüìÇ KEY FILES & DIRECTORIES\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "key_files = {\n",
    "    \"üìä Data\": [\n",
    "        \"data/sample_data.csv - Main training dataset\",\n",
    "        \"build_production_dataset.py - Dataset generation script\"\n",
    "    ],\n",
    "    \"üß† Models\": [\n",
    "        \"src/train_cnn_lstm.py - CNN-LSTM training pipeline\",\n",
    "        \"models/ - Saved model checkpoints\",\n",
    "        \"src/models/cnn_lstm.py - Model architecture\"\n",
    "    ],\n",
    "    \"‚ö° Optimization\": [\n",
    "        \"src/optimization/cnn_lstm_optimization.py - Hyperparameter tuning\",\n",
    "        \"optimization_results/ - Optimization results\",\n",
    "        \"cnn_lstm_hparam_clean.ipynb - Optimization notebook\"\n",
    "    ],\n",
    "    \"ü§ñ RL Agents\": [\n",
    "        \"src/train_rl.py - RL agent training\",\n",
    "        \"src/agents/td3_agent.py - TD3 implementation\",\n",
    "        \"src/envs/trading_env.py - Trading environment\"\n",
    "    ],\n",
    "    \"üß™ Testing\": [\n",
    "        \"tests/ - Unit and integration tests\",\n",
    "        \"quick_integration_test.py - Pipeline integration test\",\n",
    "        \"validate_dataset.py - Dataset validation\"\n",
    "    ],\n",
    "    \"üìö Documentation\": [\n",
    "        \"README.md - Project overview\",\n",
    "        \"ROADMAP.md - Development roadmap\",\n",
    "        \"STREAMLINED_PIPELINE_GUIDE.md - Pipeline guide\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, files in key_files.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for file in files:\n",
    "        print(f\"  ‚Ä¢ {file}\")\n",
    "\n",
    "print(\"\\n\\nüöÄ NEXT STEPS & RECOMMENDATIONS\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "next_steps = [\n",
    "    {\n",
    "        \"title\": \"üìà Model Improvement\",\n",
    "        \"tasks\": [\n",
    "            \"Run full hyperparameter optimization (30+ minutes)\",\n",
    "            \"Experiment with different model architectures\",\n",
    "            \"Add more features (volume indicators, market sentiment)\",\n",
    "            \"Implement ensemble methods for better predictions\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"ü§ñ RL Enhancement\",\n",
    "        \"tasks\": [\n",
    "            \"Train agents for longer periods (1000+ episodes)\",\n",
    "            \"Implement portfolio optimization\",\n",
    "            \"Add risk management constraints\",\n",
    "            \"Test different reward functions\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"üìä Backtesting\",\n",
    "        \"tasks\": [\n",
    "            \"Implement comprehensive backtesting framework\",\n",
    "            \"Calculate Sharpe ratio, max drawdown, other metrics\",\n",
    "            \"Test on out-of-sample data\",\n",
    "            \"Compare against buy-and-hold baseline\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"üî¥ Production Deployment\",\n",
    "        \"tasks\": [\n",
    "            \"Set up real-time data feeds (Alpha Vantage, IEX, etc.)\",\n",
    "            \"Integrate with broker APIs (Alpaca, Interactive Brokers)\",\n",
    "            \"Implement monitoring and alerting\",\n",
    "            \"Start with paper trading validation\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"üõ†Ô∏è Infrastructure\",\n",
    "        \"tasks\": [\n",
    "            \"Set up containerized deployment (Docker/Kubernetes)\",\n",
    "            \"Implement database for trade logging\",\n",
    "            \"Add web dashboard for monitoring\",\n",
    "            \"Set up automated model retraining\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"\\n{step['title']}:\")\n",
    "    for task in step['tasks']:\n",
    "        print(f\"  ‚Ä¢ {task}\")\n",
    "\n",
    "print(\"\\n\\nüéâ CONGRATULATIONS!\")\n",
    "print(\"=\"*20)\n",
    "print(\"You have successfully completed the end-to-end trading RL agent pipeline!\")\n",
    "print(\"\\nYou now have:\")\n",
    "print(\"‚úÖ A comprehensive trading dataset\")\n",
    "print(\"‚úÖ Trained CNN-LSTM prediction models\")\n",
    "print(\"‚úÖ Reinforcement learning trading agents\")\n",
    "print(\"‚úÖ Validated integration pipeline\")\n",
    "print(\"‚úÖ Production-ready setup\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT DISCLAIMERS:\")\n",
    "print(\"‚Ä¢ This is for educational and research purposes only\")\n",
    "print(\"‚Ä¢ Always validate strategies thoroughly before live trading\")\n",
    "print(\"‚Ä¢ Start with paper trading to test performance\")\n",
    "print(\"‚Ä¢ Never risk more than you can afford to lose\")\n",
    "print(\"‚Ä¢ Consider consulting with financial advisors\")\n",
    "\n",
    "print(\"\\nüöÄ Happy Trading! üìà\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4fa428",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Appendix: Manual Commands\n",
    "\n",
    "If you prefer to run components manually, here are the key commands:\n",
    "\n",
    "### Dataset Generation\n",
    "```bash\n",
    "python build_production_dataset.py\n",
    "python validate_dataset.py\n",
    "```\n",
    "\n",
    "### Model Training\n",
    "```bash\n",
    "python src/train_cnn_lstm.py\n",
    "python src/optimization/cnn_lstm_optimization.py\n",
    "```\n",
    "\n",
    "### RL Agent Training\n",
    "```bash\n",
    "# SAC (Ray RLlib)\n",
    "python src/train_rl.py --data data/sample_data.csv --model-path models/cnn_lstm_baseline.pth\n",
    "\n",
    "# TD3 (Custom)\n",
    "python src/train_rl.py --agent td3 --data data/sample_data.csv\n",
    "```\n",
    "\n",
    "### Testing\n",
    "```bash\n",
    "python -m pytest tests/ -v\n",
    "python quick_integration_test.py\n",
    "```\n",
    "\n",
    "### Live Trading Setup\n",
    "```bash\n",
    "# Configure data feeds, broker APIs, and monitoring\n",
    "# Start with paper trading validation\n",
    "# Implement risk management controls\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìû Support & Resources\n",
    "\n",
    "- **Documentation**: See `docs/` directory for detailed guides\n",
    "- **Issues**: Check existing tests and error logs\n",
    "- **Roadmap**: See `ROADMAP.md` for development phases\n",
    "- **Best Practices**: Follow `docs/NOTEBOOK_BEST_PRACTICES.md`\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ This completes the comprehensive end-to-end trading RL agent pipeline walkthrough!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
